---
title: Stochastic Modeling and Optimization Problemset 3
author: Aimee Barciauskas, Andreas Lloyd, Francis Thomas Moynihan IV, and Seda Yilmaz
date: 17 March 2016
output: pdf_document
---

# Problem 1

Show $\Delta J_{k+1}(x_{k}) \leq \Delta J_{k}(x_{k})$

Given $\Delta J_{k} (x_{k}+1) \leq \Delta J_{k}(x_{k})$

By definition of $\Delta$:

$$J_{k}(x_{k}+1) - J_{k}(x_{k}) \leq \Delta J_{k}(x_{k})$$

Using the definition of $J_{k}(x+i)$ from the problem:

$$\big(r_{k}(d^{*}) - d^{*}\Delta J_{k+1}(x_{k}+1) + J_{k+1}(x_{k}+1)\big) - \big(r_{k}(d^{*}) - d^{*} \Delta J_{k+1}(x_{k}) + J_{k+1}(x_{k})\big) \leq \Delta J_{k}(x_{k})$$

The $r_{k}(d^{*})$ terms cancel and grouping $d^{*}$ terms simplifies:

$$d^{*}\big(\Delta J_{k+1}(x_{k}) - \Delta J_{k+1} (x_{k}+1)\big) + \Delta J_{k+1}(x_{k}+1) \leq \Delta J_{k}(x_{k})$$

$$d^{*} \Delta J_{k+1} (x_{k}) + (1-d^{*})\Delta J_{k+1}(x_{k} + 1) \leq \Delta J_{k}(x_{k})$$

Swapping sides of $d^{*}$:

$$\Delta J_{k}(x_{k}) + (d^{*} - 1)\Delta J_{k+1}(x_{k} + 1) \leq d^{*}\Delta J_{k}(x_{k})$$

$$(d^{*} - 1)\Delta J_{k+1}(x_{k}+1) \leq (d^{*}-1)\Delta J_{k}(x_{k}) $$

The $d^{*} - 1$ on both sides term cancels:

$$\Delta J_{k+1}(x_{k}) \leq \Delta J_{k}(x_{k}) $$

# Problem 2

Given definitions:

* $x_{k}$ the state vector,
* $u_{k}$ the control vector, and
* $w_{k}$ is the disturbance vector

In order for the terminal state $x_{N} = f_{N-1}(x_{N-1}, u_{N-1}) + g_{N-1}(w_{N-1})$ to be in the target set $X_{N}$, it is necessary and sufficient for $f_{N-1}(x_{N-1}, u_{N-1})$ belong to the effective target set $E_{N}$ as defined below.

*Part a*

Given a prescribed target set $X_{N} \in \mathbb{R}^{N}$, we recursively solve for $x_{k}$ in this target set. To do this, we can define the effective target set:

$$E_{N} = \bigg\{z \in \mathbb{R}^{n} : z + g_{N-1}(w_{N-1}) \in X_{N} \forall w_{N-1} \in W_{N-1} \bigg\}$$

which will only be defined given the updated target set $T_{N-1}$, defined by:

$$T_{N-1} = \bigg\{ z \in \mathbb{R}^{n} : f_{N-1}(z, u_{N-1}) \in E_{N} \text{ for some } u_{N-1} \in U_{N-1} \bigg\}$$

The DP recursion becomes:

$$E_{k+1} = \bigg\{z \in \mathbb{R}^{n} : z + g_{k}(w_{k}) \in T_{k+1} \forall w_{k} \in W_{k} \bigg\}$$

Which is used to update the target set:

$$T_{k} = \bigg\{ z \in \mathbb{R}^{n} : f_{k}(z, u_{k}) \in E_{k+1}\bigg\}$$

$$T_{N} = X_{N}$$

What follows is that for $X_{N}$ to be reachable from set $X_{k}$ of $x_{k}$ if and only if $X_{k} \in T_{k}$

*Part b*

$X_{k+1}$ is defined as before but must be contained in $X_{k}$ for all $w_{k} \in W_{k}$

The target tube is defined as:

$$\bigg\{ \big( X_{k}, k \big), k = 1, ..., N \bigg\}$$

If we consider all sets of $X_{k}$ but $X_{N}$ to be in $\mathbb{R}^{n}$, the problem is the same as stated in part 1 with the additional requirement that all $x_{k} \in X_{k}$

To initialize the recursion, define the modeified target set:

$$X_{N-1}^{*} = T_{N-1} \cap X_{N-1}$$

it is necessary and sufficient that

$$x_{N-1} \in X_{N-1}^{*} \text{ and } T_{N-1}$$

e.g. $x_{N-1}$ is from the modified target set.

The DP recursion is:

$$E_{k+1}^{*} = \bigg\{z \in \mathbb{R}^{n} : z + g_{k}(w_{k}) \in X_{k+1}^{*} \forall w_{k} \in W_{k} \bigg\}$$

$$T_{k}^{*} = \bigg\{ z \in \mathbb{R}^{n} : f_{k}(z, u_{k}) \in E_{k+1} \text{ for some } u_{k} \in U_{k} \bigg\}$$

$$X_{k}^{*} = T_{k}^{*} \cap X_{k}$$

$$X_{N}^{*} = X_{N}$$

The target tube $\bigg\{X_{j}, j ; j = k + 1, ..., N \bigg\}$ is reachable at time $k$ if and only if $x_{k} \in T_{k}^{*}$ e.g. the target tube is reachable if and only if $X_{0} \subset T_{0}^{*}$

# Problem 3

The set up of the problem is that we have the DP algorithm of the form
$$ J_{N}(x_{N}, y_{N}) = x^{'}_{N} Q_{N} x_{N}
J_{k}(x_{k}, y_{k}) = min E \{ x^{'}_{k} Q_{k} x_{k} + u^{'}_{k} R_{k} u_{k} +\Sigma p_{i}^{k+1}  J_{k+1}(x_{k+1}, i) | y_{k}\}$$

Then we need to prove $J_{k}(x_{k}, y_{k}) =  x^{'}_{k} K_{k} x_{k} + x^{'}_{k}b_{k}(y_{k}) + c_{k}(y_{k})$ by induction. We start by assuming it is true for $J_{k+1}(x_{k+1}, y_{k+1})$, then for $J_{k}(x_{k}, y_{k})$
$$J_{k}(x_{k}, y_{k}) = min E \{ x^{'}_{k} Q_{k} x_{k} + u^{'}_{k} R_{k} u_{k} +\Sigma p_{i}^{k+1}  [x^{'}_{k+1} K_{k+1} x_{k+1} + x^{'}_{k+1}b_{k+1}(i) + c_{k+1}(i)] | y_{k}\} = 
x^{'}_{k} Q_{k} x_{k} + min\{ u^{'}_{k} R_{k} u_{k} + E \{(A_{k}x_{k} + B_{k}u_{k} + w_{k})^{'}K_{k+1}(A_{k}x_{k} + B_{k}u_{k} + w_{k}) + (A_{k}x_{k} + B_{k}u_{k} + w_{k})^{'} b_{k+1} | y_{k}\} \gamma_{k+1}\}$$

Where we have absorbed the sum terms to pull out the terms with $A_{k}$ and $B_{k}$, and $b_{k+1} = \Sigma p_{i}^{k+1}b_{k+1}(i)$, $\gamma_{k+1} = \Sigma p_{i}^{k+1}c_{k+1}(i)$. The expectations are over $w_{k}$, and the minimisation over $u_{k}$, as normal, so further simplifications can be made to give us a term dependent on $u_{k}$ and one not. The term dependent on $u_{k}$ is the one that gives our optimal control law,
$$min \{u^{'}_{k} (R_{k}+ B^{'}_{k}K_{k+1}B_{k})u_{k} + 2u^{'}_{k}B^{'}_{k}K_{k+1}A_{k}x_{k} + 2u^{'}_{k}B^{'}_{k}K_{k+1}E[w_{k}|y_{k}] + u^{'}_{k}B^{'}_{k}b_{k+1}\}$$

The minimum here can be found by differentiation as the first term is positive,
$$ 2(R_{k}+ B^{'}_{k}K_{k+1}B_{k})u^{*}_{k} +2B^{'}_{k}K_{k+1}(A_{k}x_{k} + E[w_{k}|y_{k}] + B^{'}_{k}b_{k+1}) = 0 $$
This gives an optimal control law as
$$u^{*}_{k} = -(R_{k}+ B^{'}_{k}K_{k+1}B_{k})^{-1}B^{'}_{k}K_{k+1}(A_{k}x_{k} + E[w_{k}|y_{k}]) - \frac{1}{2}(R_{k}+ B^{'}_{k}K_{k+1}B_{k})^{-1}B^{'}_{k}b_{k+1}$$
And the final term can be referred to as $\alpha_{k}$. 

Substituting this back in to the equation gives us a set of quadratic, linear and constant terms in $x_{k}$, which can be re-formulated in the desired form to give,
$$J_{k}(x_{k}, y_{k}) = x^{'}_{k} K_{k} x_{k} + x^{'}_{k}b_{k}(y_{k}) + c_{k}(y_{k})$$
which is the desired form. So the proof is complete and the desired form of the optimal control was found along the way.

# Problem 4

Now the aim is to prove that the optimal control law depends linearly on the state. Considering the cost function, the formulation of the DP algorithm is
$$ J_{N}(X_{N}) = e^{x^{2}_{N}}
J_{k}(x_{k}) = min \{e^{x^{2}_{k} + ru^{2}_{k}}E(J_{k+1}(x_{k+1}))\}$$
and we want to proceed by proving $J_{k}(x_{k})=\alpha_{k}e^{\beta_{k}x^{2}_{k}}$. Assume it is true for $J_{k+1}(x_{k+1}$ and considering $x_{k+1} = a_{k}x_{k} + b_{k}u_{k} + w_{k}$, then for $J_{k}(x_{k})$,
$$J_{k}(x_{k}) = min \{e^{x^{2}_{k} + ru^{2}_{k}}E(\alpha_{k+1}e^{\beta_{k+1}x^{2}_{k+1}})\} = 
min \{e^{x^{2}_{k} + ru^{2}_{k}}\frac{\alpha_{k+1}}{\sqrt{Z_{k+1}}}e^{\frac{\beta_{k+1}(a_{k}x_{k} + b_{k}u_{k})^{2}}{Z_{k+1}}}\}$$
where the given relation was used and $Z_{k+1} = 1 - 2\beta_{k+1}\sigma^{2}$ was used as a dummy variable to make things look a bit neater.

Now, the only non-constant terms wrt $u_{k}$ are the exponentials, so the minimisation is a simple problem of differentiating the exponential parts to give,
$$2ru^{*}_{k} +  2\frac{b_{k}u^{*}_{k}}{Z_{k+1}} = 0$$

This is clearly a linear equation, so we have $u^{*}_{k} = \gamma_{k}x_{k}$, as desired. Similar to the last problem, subsituting in the form for $u^{*}_{k}$ will complete the induction, as there is a $x_{k}^{2}$ term introduced in the exponential. We can then absorb all remaining constants and re-using the relation given by the question, a form of $J_{k}(x_{k})=\alpha_{k}e^{\beta_{k}x^{2}_{k}}$ is returned simply.


# Problem 5

Below we define the function called `get.states` which returns a $2\times 1$ vector of states from time $1$ to $N$.

The plots which follow are given 2 per sub-problem, 1 each of the values of $x$ state vector. In other words, one plot for each $x_{1}$ and $x_{2}$ over time given the different system arguments. The blue line is always for the "small" or "non-riccardi" version of the system and the red line is always for the "large" or "riccardi" of the system.

```{r message=FALSE, warning=FALSE}
if (!require('assertthat')) install.packages('asserthat')
if (!require('matrixcalc')) install.packages('matrixcalc')
if (!require('mvtnorm')) install.packages('mvtnorm')
if (!require('Matrix')) install.packages('Matrix')

# Initial values for x, A, B, C and R
# go to 101 since R's indexing starts at 1
# the first element of everything is associted with t = 0,
# and the last element (the 101th element) with t = 100
get.states <- function(N = 101,
                       x0 = c(1,1),
                       A = matrix(c(0,1,1,0), nrow = 2, ncol = 2),
                       B = matrix(c(1,1,7,1), nrow = 2, ncol = 2),
                       C = c(2,1),
                       Q = C %*% t(C),
                       R = diag(x = c(2,3)),
                       ws = rmvnorm(N-1, mean = c(0,0), sigma = diag(x = c(0.1,0.2))),
                       riccardi = FALSE) {
  set.seed(321)
  are_equal(rankMatrix(cbind(A, A%*%B))[1], max(nrow(A), ncol(A)))
  assert_that(is.positive.definite(R))
  # Initialize stores for x, L and K
  x.mat <- matrix(NA, nrow = N, ncol = length(x0))
  # store the initial state x0 as the first element in x
  x.mat[1,] <- x0
  #L.mat <- matrix(NA, nrow = N, ncol = length(x0))
  L.list <- list()
  # K's are (length of C) x (length of C), e.g. NxN
  K.list <- list()
  
  # Terminal K_N = C'C
  K.list[[N]] <- Q
  
  # for t = 99 to 0, solve for K and L
  # R indices 100 to 1
  for (t in (N-1):1) {
    # if riccardi, stop updating K once it converges
    K.tplus1 <- K.list[t+1][[1]]
    # check to see if we have converged
    K.tplus2 <- K.list[t+2][[1]]
    K.list[[t]] <- if ((riccardi == FALSE) || (!(t == N-1) && !(K.tplus2 == K.tplus1))) {
      # break into parts for readability
      inner <- K.tplus1 - K.tplus1 %*% B %*% solve(R + t(B) %*% K.tplus1 %*% B) %*% t(B) %*% K.tplus1
      t(A) %*% (inner) %*% A + Q
    } else {
      K.tplus1
    }
    L.list[[t]] <- -solve(R + t(B)%*%K.tplus1%*%B) %*% t(B) %*% K.tplus1 %*% A
  }
  
  # Use L's to solve for optimal control
  # from t = 1 to 100 (e.g. R indices 2 to 101)
  # first component of xs is x0 -> x0, so need to index from t+1
  for (t in 2:N) {
    lastperiod <- t-1
    x.lastperiod <- x.mat[lastperiod,]
    L.lastperiod <- L.list[[lastperiod]]
    w.lastperiod <- ws[lastperiod,]
    # solve for x_{k+1}
    x.mat[t,] <- A %*% x.lastperiod + B %*% (L.lastperiod %*% x.lastperiod) + w.lastperiod
  }
  
  return(list(x.matrix = x.mat))
}
```

We use this function with modified arguments below:

**(i) Fix $R$ and $x_{0}$, and compare the behavior of the system for two covariance matrices for the disturbances, one “much larger” than the other, under optimal control (given by the discrete-time Riccati equation) small**

```{r}
first.q.small <- get.states()
# explicitly state N for clarity
N = 101
first.q.large <- get.states(N = N, ws = rmvnorm(N-1, mean = c(0,0), sigma = diag(x = c(1.2,3.2))))

min.x1 <- min(first.q.large$x.matrix[,1])
max.x1 <- max(first.q.large$x.matrix[,1])
plot(first.q.small$x.matrix[,1],
     type = 'l',
     col = 'blue',
     ylim = c(min.x1, max.x1),
     ylab = 'x1')
lines(first.q.large$x.matrix[,1], type = 'l', col = 'red')

min.x2 <- min(first.q.large$x.matrix[,2])
max.x2 <- max(first.q.large$x.matrix[,2])
plot(first.q.small$x.matrix[,2],
     type = 'l',
     col = 'blue',
     ylim = c(min.x2, max.x2),
     ylab = 'x2')
lines(first.q.large$x.matrix[,2], type = 'l', col = 'red')
```

**(ii) Fix $R$ and $D$, and compare the behavior of the system for two initial conditions; one "much larger" than the other, under optimal control;**

```{r}
second.q.small <- get.states()
second.q.large <- get.states(x0 = c(21,43))
min.x1 <- min(second.q.small$x.matrix[,1])
max.x1 <- max(second.q.large$x.matrix[,1])
plot(second.q.small$x.matrix[,1],
     type = 'l',
     col = 'blue',
     ylim = c(min.x1-2, max.x1),
     ylab = 'x1')
lines(second.q.large$x.matrix[,1], type = 'l', col = 'red')

min.x2 <- min(second.q.small$x.matrix[,2])
max.x2 <- max(second.q.large$x.matrix[,2])
plot(second.q.small$x.matrix[,2],
     type = 'l',
     col = 'blue',
     ylim = c(min.x2-10, max.x2),
     ylab = 'x2')
lines(second.q.large$x.matrix[,2], type = 'l', col = 'red')
```

**(iii) Fix $x_{0}$ and $D$, and compare the behavior of the system for two input-cost matrices, one "much larger" than the other, under optimal control;**

```{r}
third.q.small <- get.states()
third.q.large <- get.states(R = diag(x = c(99,23)))
min.x1 <- min(third.q.small$x.matrix[,1])
max.x1 <- max(third.q.large$x.matrix[,1])
plot(third.q.small$x.matrix[,1],
     type = 'l',
     col = 'blue',
     ylim = c(min.x1, max.x1),
     ylab = 'x1')
lines(third.q.large$x.matrix[,1]+1e-2, type = 'l', col = 'red')

min.x2 <- min(third.q.small$x.matrix[,2])
max.x2 <- max(third.q.large$x.matrix[,2])
plot(third.q.small$x.matrix[,2],
     type = 'l',
     col = 'blue',
     ylim = c(min.x2, max.x2),
     ylab = 'x2')
lines(third.q.large$x.matrix[,2]+1e-2, type = 'l', col = 'red')
```

**(iv) Fix $R$, $x_{0}$, and $D$, and compare the behavior of the system under optimal control vs. steady-state control (given by the algebraic Riccati equation).**

```{r}
fourth.q.normal <- get.states(riccardi = FALSE)
fourth.q.riccardi <- get.states(riccardi = TRUE)
plot(fourth.q.normal$x.matrix[,1],
     type = 'l',
     col = 'blue',
     ylab = 'x1')
lines(fourth.q.riccardi$x.matrix[,1]+1e-2, type = 'l', col = 'red')

plot(fourth.q.normal$x.matrix[,2],
     type = 'l',
     col = 'blue',
     ylab = 'x2')
lines(fourth.q.riccardi$x.matrix[,2]+1e-2, type = 'l', col = 'red')
```
